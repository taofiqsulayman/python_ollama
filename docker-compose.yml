version: '3.8'

services:
  app:
    build: .
    privileged: true  # Add this for Amazon Linux 2023
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              driver: nvidia
              count: all
              device_ids: ['0']
    ports:
      - "8000:8000"  # FastAPI
      - "8501:8501"  # Streamlit
    environment:
      - DATABASE_URL=postgresql://fileprocessor:${DB_PASSWORD}@db:5432/fileprocessor
      - PYTHONPATH=/app
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ollama_models:/root/.ollama  # Persist Ollama models
      - /usr/bin/nvidia-smi:/usr/bin/nvidia-smi  # Map nvidia-smi
      - /usr/lib64/libnvidia-ml.so:/usr/lib64/libnvidia-ml.so  # Map NVIDIA libraries
      - /usr/lib64/libnvidia-ml.so.1:/usr/lib64/libnvidia-ml.so.1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      db:
        condition: service_healthy
    networks:
      - app_network

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=fileprocessor
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=fileprocessor
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U fileprocessor"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app_network

networks:
  app_network:
    driver: bridge

volumes:
  postgres_data:
  ollama_models:
